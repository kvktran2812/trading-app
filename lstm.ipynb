{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f20c7cb3-66e0-4a65-9ac2-cc9bbb89f7b8",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5b30202-7959-46fa-b858-b91a14eac504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ea50e1-a5b3-4fdc-86cb-77dde04ced46",
   "metadata": {},
   "source": [
    "# Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01ac0573-ac45-4523-821f-242ce13830ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets for stock \n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, ticker, interval=\"1d\", period=\"max\", n_windows=64):\n",
    "        self.interval = interval\n",
    "        self.period = period\n",
    "        self.n_windows = n_windows\n",
    "\n",
    "        # Retrieve raw data\n",
    "        self.ticker = yf.Ticker(ticker)\n",
    "        data = self.ticker.history(interval=\"1d\", period=\"max\")\n",
    "        data = data.reset_index()\n",
    "        data = data.drop([\"Dividends\", \"Stock Splits\", \"Volume\", \"Date\"], axis=1)\n",
    "        data = data.to_numpy()\n",
    "        self.data = data\n",
    "\n",
    "        # Calculate return of interest\n",
    "        n = data.shape[0]\n",
    "        shape = data.shape\n",
    "        rot = np.zeros((shape[0] - 1, shape[1]))\n",
    "        \n",
    "        for i in range(n-1):\n",
    "            rot[i] = (data[i+1] - data[i]) / data[i]\n",
    "\n",
    "        # Preprocessing to X and y\n",
    "        self.X = np.zeros((len(data) - n_windows, n_windows, 4))\n",
    "        self.y = np.zeros((len(data) - n_windows, 4))\n",
    "        assert len(self.X) == len(self.y), \"Size of X and y is not equal\"\n",
    "\n",
    "        for i in range(rot[:,0].shape[0] - (n_windows + 1)):\n",
    "            self.X[i] = rot[i:i+n_windows,:]\n",
    "            self.y[i] = rot[i+n_windows,:]\n",
    "        self.rot = rot\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "994fc870-4c41-4d45-bdce-3d375c7fbfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128, 4])\n",
      "torch.Size([64, 4])\n"
     ]
    }
   ],
   "source": [
    "# DataLoader\n",
    "training_data = StockDataset(\"MSFT\", n_windows=128)\n",
    "dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "\n",
    "train_features, train_predition = next(iter(dataloader))\n",
    "print(train_features.shape)\n",
    "print(train_predition.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f4a57c-a915-48bf-b0cb-3cfa188238f8",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0afa7579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128, 4])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = torch.Tensor(train_features).to(torch.float32)\n",
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5033691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.randn(64, 1, 4)\n",
    "\n",
    "test_layer = nn.LSTM(input_size = 4, hidden_size = 64, num_layers=1, batch_first=True)\n",
    "\n",
    "h_0 = torch.zeros(1, 64, 64).requires_grad_()\n",
    "c_0 = torch.zeros(1, 64, 64).requires_grad_()\n",
    "\n",
    "out, (hn, cn) = test_layer(test_input, (h_0, c_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e47c1015-4ecf-470b-9403-db9cb2b5f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, input_size=4, hidden_size=128, num_layers=1, output_size=4):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states with zeros\n",
    "        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_()\n",
    "        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_()\n",
    "        \n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through several iterations\n",
    "        out, (hn, cn) = self.lstm(x, (h_0.detach(), c_0.detach()))\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6d49d1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.0004037\n",
      "Epoch [20/100], Loss: 0.0002688\n",
      "Epoch [30/100], Loss: 0.0002163\n",
      "Epoch [40/100], Loss: 0.0002016\n",
      "Epoch [50/100], Loss: 0.0001913\n",
      "Epoch [60/100], Loss: 0.0001843\n",
      "Epoch [70/100], Loss: 0.0001774\n",
      "Epoch [80/100], Loss: 0.0001706\n",
      "Epoch [90/100], Loss: 0.0001636\n",
      "Epoch [100/100], Loss: 0.0001568\n"
     ]
    }
   ],
   "source": [
    "X = torch.Tensor(train_features).to(torch.float32)\n",
    "y = torch.Tensor(train_predition).to(torch.float32)\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "model = LSTM_Model(num_layers=1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X)\n",
    "    loss = criterion(output.squeeze(), y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f'Epoch [{i+1}/{num_epochs}], Loss: {loss.item():.7f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a356bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
